{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb459e21"
      },
      "source": [
        "# Package Install\n",
        "\n",
        "The installation of the packages forms the technological basis necessary for advanced analysis: `tensorflow` provides deep learning functionality with Keras API to build and train a model; `numpy` provides means for efficient operations on numerical arrays and mathematical computations in general, essential for scientific computing; `pandas` allows structured data manipulation and tabular analysis; `matplotlib` and `seaborn` provide statistical visualisations and graphical representations that are essential in exploratory data analysis and result communication; `scikit-learn` provides utilities for data preprocessing, model evaluation metrics, and data partition functions; and `pillow` allows loading and manipulating images, which is crucial for vision tasks and exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b658612f"
      },
      "source": [
        "Uncomment the line that corresponds to your Kernel of choice if packages have not been installed yet (Step in README.md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6b956df"
      },
      "outputs": [],
      "source": [
        "#%conda install -c conda-forge tensorflow numpy pandas matplotlib seaborn pillow scikit-learn tqdm editdistance pyspark keras glob\n",
        "%pip install --upgrade pip setuptools wheel\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94656dd7",
      "metadata": {},
      "source": [
        "# Import Libraries\n",
        "\n",
        "The import statements are strategically organized in such a way to set up an all-encompassing analytical and modeling environment for the project on recognition of CAPTCHA. Core data handling relies on `numpy` for numerical array manipulation and `pandas` for structured DataFrame operations. These libraries support dataset preparation, statistical summaries, and tabular exploration.\n",
        "\n",
        "Computer vision and deep learning capability is supplied by `tensorflow` and `tensorflow.keras`, which provide the comprehensive framework for building, compiling, training, and optimizing a model. This includes convolutional layers, activation functions, callbacks to control training, and utilities for saving and loading models.\n",
        "\n",
        "Visualization and diagnostic evaluation is aided by `matplotlib.pyplot` for customizable plots and `seaborn` for improved statistical graphics that allow for the clear presentation of distributions, model performance curves, and findings from exploratory analysis. `Scikit-learn` provides the necessary functionalities related to model evaluation, such as confusion matrices, ROC and precision-recall curve calculation, AUC score, and other metrics applied for predictive performance evaluation. This module also includes functions for dividing datasets into the correct proportions of training, validation, and test subsets.\n",
        "\n",
        "It also includes image processing functionalities using the Python Imaging Library `PIL` for loading, converting, and inspecting the CAPTCHA images. This allows the inspection of image dimensions, color patterns, and other image artifacts that could affect model behavior. Additional supporting libraries include `tqdm` for progress monitoring, `glob` and `pathlib` for managing file paths, and configuration of random seeds for reproducibility in Python, NumPy, and TensorFlow. Together, these complete an environment for data preparation, exploratory data analysis, development of convolutional models, and empirical evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bd8dcdf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import glob\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "from pyspark.sql.functions import col, regexp_extract, input_file_name, explode, split, count, desc\n",
        "import pyspark.sql.functions as F\n",
        "import threading\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5a8061",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_spark():\n",
        "    global spark\n",
        "    try:\n",
        "        spark = (\n",
        "            SparkSession.builder\n",
        "            .appName(\"CAPTCHA_Solver\")\n",
        "            .master(\"local[*]\")\n",
        "            .getOrCreate()\n",
        "        )\n",
        "        spark.sparkContext.setLogLevel(\"WARN\")\n",
        "    except Exception:\n",
        "        spark = \"\"\n",
        "\n",
        "# Timeout in seconds\n",
        "TIMEOUT = 20\n",
        "\n",
        "thread = threading.Thread(target=build_spark)\n",
        "thread.start()\n",
        "thread.join(TIMEOUT)\n",
        "\n",
        "if thread.is_alive():\n",
        "    print(f\"SparkSession creation exceeded {TIMEOUT} seconds. Skipping...\")\n",
        "    spark = \"\"\n",
        "else:\n",
        "    if spark:\n",
        "        print(\"SparkSession created successfully.\")\n",
        "    else:\n",
        "        print(\"SparkSession failed to create.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d151290",
      "metadata": {},
      "source": [
        "# Project Constants & Configuration\n",
        "\n",
        "This section provides for the foundational parameters and directory structures, necessary for managing and training on the large-scale CAPTCHA image dataset. For this project, a dataset consisting of over one million images will be used. Access can be obtained through the following links:\n",
        "- **Kaggle Link:** [1M Big CAPTCHA Dataset](https://www.kaggle.com/datasets/muzzamalhameed/1m-big-captcha-dataset/data?select=147EAhiwL4CPT5ez8LDUakKsIO5BbiUNS)\n",
        "\n",
        "- **Google Drive Link:** [1M Big CAPTCHA Dataset](https://drive.google.com/drive/folders/147EAhiwL4CPT5ez8LDUakKsIO5BbiUNS?usp=drive_link)\n",
        "\n",
        "- **Dataset Link:** [Dataset OneDrive Folder](https://advtechonline-my.sharepoint.com/:f:/g/personal/st10204772_vcconnect_edu_za/IgCZ5bCXuRigRrgs4mNaeBcFAd4aw2KVIZfDxlmBo4CNgKw?e=Q3DNsh)\n",
        "\n",
        "Other key configuration parameters are defined to ensure reproducibility, including a fixed random state to achieve identical splits of the data and hence identical training. Image preprocessing settings, such as target height, width, and batch size, are standardized across all input to provide consistent training.\n",
        "\n",
        "Training and evaluation parameters, including the number of epochs, learning rate, and proportions of data reserved for testing and validation, are defined to guide model development and performance assessment. The directories are set up such that outputs, visualizations, and metrics are systematically stored. It provides clarity in terms of the workflow's reproducibility for maintaining results, tracking experiments, and general project organization. This will ensure that the dataset and further processes are handled efficiently and consistently throughout the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7cbe41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image preprocessing parameters\n",
        "IMG_HEIGHT = 100\n",
        "IMG_WIDTH = 200\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.005\n",
        "TEST_SPLIT = 0.2\n",
        "VAL_SPLIT = 0.1\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Results directory\n",
        "BASE_DIR = Path().resolve()\n",
        "DATA_DIR = BASE_DIR / \"1M_Big_Captcha_Dataset\"\n",
        "RESULTS_DIR = \"results/\"\n",
        "FIGURE_DIR = \"results/figures/\"\n",
        "METRIC_DIR = \"results/metrics/\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
        "os.makedirs(METRIC_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e254bc4c"
      },
      "source": [
        "# Data Loading and Preprocessing Classes\n",
        "\n",
        "The following section defines a comprehensive and extensible framework for loading, preprocessing, and preparing large-scale CAPTCHA image datasets for deep learning training. At its core, the `CAPTCHADataLoader` class is designed to handle such significant dataset challenges as memory efficiency, label encoding, and full-colour image preprocessing to prepare the data for CNN training.\n",
        "\n",
        "The flexible methods for reading images from disk are provided by the loader. If `PySpark` is available, this can be used to distribute the loading across multiple nodes, hence handling very large datasets much more quickly. Fallbacks to using `TensorFlow`-based loading are done in the case of the unavailability of `PySpark`, whereby recursive directory scanning can be utilized to identify images, while keeping a structured mapping of paths to labels. This ensures that this pipeline can be applied both within large-scale environments and more resource-constrained ones.\n",
        "\n",
        "Once images are found, the loader extracts labels from the filename, removing file extensions automatically. It then constructs character-to-number and number-to-character mappings, of vital importance in encoding textual labels into a numerical form that neural networks can be trained on. Multi-character labels are one-hot encoded for each character position in the label, allowing the model to predict sequences of characters simultaneously. In addition, a decoding method is provided to translate model outputs back into text that will be more readable, making it easier to assess and verify predictions.\n",
        "\n",
        "Preprocessing is done on-the-fly in a manner to balance memory usage with training efficiency. Images are resized to consistent dimensions, normalized in the [0,1] range, and kept in RGB color to preserve vital visual features critical to state-of-the-art recognition of the CAPTCHAs. Optional, but particularly useful during training, is augmentation that randomly adjusts brightness, contrast, and saturation while adding Gaussian noise to simulate the various distortions and variability common in CAPTCHA images. Such augmentations help the model generalize well to unseen CAPTCHAs.\n",
        "The `create_dataset` function constructs an optimized TensorFlow `tf.data.Dataset` pipeline that encompasses all of the major steps: loading images, preprocessing, and encoding labels. It supports batching, shuffling, caching, and prefetching to efficiently manage massive-scale CAPTCHA dataset preparation with minimal I/O bottlenecks in order to support high-performance training. This infrastructure deals with lazy loading, on-the-fly preprocessing, augmentation, and provides an efficient data pipeline that is all inclusive in preparing large-scale CAPTCHA datasets. It assures reproducibility, scalability, and robustness, appropriate for CNN-based model training that demands accuracy and efficiency necessary when dealing with complex multi-character CAPTCHA recognition tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cfb157e"
      },
      "outputs": [],
      "source": [
        "class CAPTCHADataLoader:\n",
        "    \"\"\"\n",
        "    Handles loading of CAPTCHA dataset.\n",
        "    Contains Tensorflow dataset loading fallback if Pyspark fails.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, img_height=IMG_HEIGHT, img_width=IMG_WIDTH):\n",
        "        # Initialise class attributes\n",
        "        self.image_dir = image_dir\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.characters = None\n",
        "        self.char_to_num = None\n",
        "        self.num_to_char = None\n",
        "        self.data_df = None\n",
        "        self.load_method = None\n",
        "\n",
        "    # Loads dataset using Spark\n",
        "    def load_with_spark(self):\n",
        "        if spark is None:\n",
        "            return self.load_with_tensorflow()\n",
        "\n",
        "        try:\n",
        "            image_extensions = ['jpg', 'jpeg', 'png']\n",
        "            all_data = []\n",
        "\n",
        "            for ext in image_extensions:\n",
        "                pattern = os.path.join(self.image_dir, f\"*.{ext}\")\n",
        "                files_rdd = spark.sparkContext.binaryFiles(pattern)\n",
        "                paths = files_rdd.map(lambda x: x[0]).collect()\n",
        "\n",
        "                for path in paths:\n",
        "                    filename = os.path.basename(path)\n",
        "                    label = os.path.splitext(filename)[0]\n",
        "                    all_data.append((path, filename, label, len(label)))\n",
        "\n",
        "            if not all_data:\n",
        "                raise Exception(\"No images found with PySpark.\")\n",
        "\n",
        "            schema = StructType([\n",
        "                StructField(\"image_path\", StringType(), True),\n",
        "                StructField(\"filename\", StringType(), True),\n",
        "                StructField(\"label\", StringType(), True),\n",
        "                StructField(\"label_length\", IntegerType(), True)\n",
        "            ])\n",
        "            spark_df = spark.createDataFrame(all_data, schema)\n",
        "            self.data_df = spark_df.select(\"image_path\", \"label\").toPandas()\n",
        "            self.load_method = \"pyspark\"\n",
        "            return self.data_df\n",
        "\n",
        "        # Calls function to Load images with Tensorflow if an error occurs\n",
        "        except Exception:\n",
        "            return self.load_with_tensorflow()\n",
        "\n",
        "    # Loads dataset using Tensorflow\n",
        "    def load_with_tensorflow(self):\n",
        "        image_extensions = ['jpg', 'jpeg', 'png']\n",
        "        all_paths = []\n",
        "\n",
        "        for ext in image_extensions:\n",
        "            pattern = os.path.join(self.image_dir, f\"**/*.{ext}\")\n",
        "            all_paths.extend(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        if not all_paths:\n",
        "            raise Exception(f\"No images found in {self.image_dir}\")\n",
        "\n",
        "        data = [{'image_path': path, 'label': os.path.splitext(os.path.basename(path))[0]}\n",
        "                for path in all_paths]\n",
        "\n",
        "        self.data_df = pd.DataFrame(data)\n",
        "        self.load_method = \"tensorflow\"\n",
        "        return self.data_df\n",
        "\n",
        "    def build_character_mappings(self, labels):\n",
        "        all_chars = sorted(set(''.join(labels)))\n",
        "        self.characters = all_chars\n",
        "        self.char_to_num = {char: idx for idx, char in enumerate(all_chars)}\n",
        "        self.num_to_char = {idx: char for idx, char in enumerate(all_chars)}\n",
        "\n",
        "    # gets unprocessed images straight from dataset\n",
        "    def get_data(self, limit=None, max_label_length=6):\n",
        "        # IF for some reason, the dataset has not been loaded, it calls function to load it\n",
        "        if self.data_df is None:\n",
        "            self.load_with_spark()\n",
        "\n",
        "        if self.data_df is None or self.data_df.empty:\n",
        "            return [], []\n",
        "\n",
        "        self.data_df = self.data_df[self.data_df['label'].str.len() <= max_label_length]\n",
        "\n",
        "        if limit:\n",
        "            subset = self.data_df.head(limit)\n",
        "        else:\n",
        "            subset = self.data_df\n",
        "\n",
        "        image_paths = subset['image_path'].tolist()\n",
        "        labels = subset['label'].tolist()\n",
        "\n",
        "        if self.characters is None and labels:\n",
        "            self.build_character_mappings(labels)\n",
        "\n",
        "        return image_paths, labels\n",
        "\n",
        "    # Image preprocessing function, per image\n",
        "    def preprocess_image(self, img_path):\n",
        "        \"\"\"\n",
        "        TensorFlow-native image loading and preprocessing for images.\n",
        "        \"\"\"\n",
        "        img = tf.io.read_file(img_path)\n",
        "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "        img = tf.image.resize(img, [self.img_height, self.img_width])\n",
        "        img = tf.cast(img, tf.float32) / 255.0\n",
        "        return img\n",
        "\n",
        "    # Label encoding function, per label\n",
        "    def encode_single_label(self, label):\n",
        "        encoded = [self.char_to_num.get(char, 0) for char in label]\n",
        "        return encoded\n",
        "\n",
        "    # Encoding label function for all labels\n",
        "    def encode_labels(self, labels, max_length=6):\n",
        "        if self.char_to_num is None:\n",
        "            raise ValueError(\"Character mappings not built.\")\n",
        "\n",
        "        encoded_labels = []\n",
        "        for i in range(max_length):\n",
        "            position_labels = [\n",
        "                self.char_to_num.get(label[i], 0) if i < len(label) else 0\n",
        "                for label in labels\n",
        "            ]\n",
        "            one_hot = tf.keras.utils.to_categorical(\n",
        "                position_labels, num_classes=len(self.characters))\n",
        "            encoded_labels.append(one_hot)\n",
        "\n",
        "        return encoded_labels\n",
        "\n",
        "    # Decodes predictions\n",
        "    def decode_predictions(self, predictions):\n",
        "        if self.num_to_char is None:\n",
        "            raise ValueError(\"Character mappings not built.\")\n",
        "\n",
        "        decoded_texts = []\n",
        "        batch_size = predictions[0].shape[0]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            text = \"\"\n",
        "            for char_pred in predictions:\n",
        "                char_idx = np.argmax(char_pred[i])\n",
        "                if char_idx < len(self.characters):\n",
        "                    text += self.num_to_char[char_idx]\n",
        "            decoded_texts.append(text)\n",
        "\n",
        "        return decoded_texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9fc784a"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path, img_height, img_width, is_training=False):\n",
        "    \"\"\"Load and preprocess a single CAPTCHA image.\"\"\"\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3) \n",
        "    image = tf.image.resize(image, [img_height, img_width])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    if is_training:\n",
        "        # Augmentations that help with CAPTCHA distortion\n",
        "        image = tf.image.random_brightness(image, max_delta=0.25)\n",
        "        image = tf.image.random_contrast(image, lower=0.8, upper=1.3)\n",
        "        image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
        "\n",
        "        # Slight Gaussian noise to improve robustness\n",
        "        noise = tf.random.normal(tf.shape(image), mean=0.0, stddev=0.03)\n",
        "        image = tf.clip_by_value(image + noise, 0.0, 1.0)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def create_dataset(\n",
        "    image_paths, \n",
        "    labels, \n",
        "    data_loader, \n",
        "    max_label_length=6, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True, \n",
        "    is_training=False\n",
        "):\n",
        "    \"\"\"Creates an efficient, tf.data pipeline for CAPTCHA training.\"\"\"\n",
        "    \n",
        "    num_classes = len(data_loader.characters)\n",
        "    \n",
        "    # Encode labels with padding\n",
        "    encoded_labels = [\n",
        "        [data_loader.char_to_num.get(char, data_loader.char_to_num.get(' ', 0)) \n",
        "         for char in label.ljust(max_label_length, ' ')]\n",
        "        for label in labels\n",
        "    ]\n",
        "    encoded_labels_tensor = tf.convert_to_tensor(encoded_labels, dtype=tf.int32)\n",
        "\n",
        "    def process_single_sample(image_path, encoded_chars):\n",
        "        # Load and preprocess the image\n",
        "        image = load_and_preprocess_image(\n",
        "            image_path, \n",
        "            data_loader.img_height, \n",
        "            data_loader.img_width, \n",
        "            is_training=is_training\n",
        "        )\n",
        "        # Convert each character to one-hot encoded\n",
        "        one_hot_labels = {\n",
        "            f'char_{i+1}': tf.one_hot(encoded_chars[i], num_classes)\n",
        "            for i in range(max_label_length)\n",
        "        }\n",
        "        return image, one_hot_labels\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, encoded_labels_tensor))\n",
        "    dataset = dataset.map(process_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.cache().shuffle(buffer_size=min(1000, len(image_paths)))\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f1aef97"
      },
      "source": [
        "# Exploratory Data Analysis & Loading of Dataset\n",
        "\n",
        "This section covers the initialization of the CAPTCHA dataset and an in-depth exploratory analysis in order to understand the characteristics of the data in view of training a CNN model. The **data loader** is instantiated with the dataset directory and target image dimensions, giving access to image paths and labels. A subset of 50 000 images is loaded for efficient analysis while ensuring that it is well-representative for statistical insights. The **maximum length of the label** is determined with a view to configuring the output layers of the CNN for the prediction of multiple characters.\n",
        "\n",
        "To have a better view of the frequency distribution of every character in this dataset, the character-level statistics are computed. The **Counter** object captures occurrences, and the top 20 most frequent characters are visualized in a bar plot that helps to know the potential class imbalances, which are critical for model training and evaluation. Summary statistics, including the total number of unique characters and most common character occurrence, help in designing the output layer and loss functions.\n",
        "\n",
        "Sample CAPTCHA images are provided to give a qualitative overview of the dataset and the variability in the placements of the characters, the fonts used, and the background patterns. This also serves to visually inspect problems that the CNN might face, such as distorted characters or overlapping elements. Image dimensions and channel information for each image is sampled over a subset to ensure image height and width are congruent with channel depth. Unique channel counts and the most common channel configuration are printed out to make sure the imagery comes in a format expected by the model.\n",
        "\n",
        "Distributions of pixel intensities along the Red, Green, and Blue channels were analyzed by flattening pixel values and plotting histograms. This gives insight into the brightness and contrast and possible color biases in the dataset. Channel-wise statistics like mean, standard deviation, minimum, and maximum values give a quantitative measure of pixel intensity variation and thus guide preprocessing steps like normalization or adjusting contrast. These histograms and statistics would help identify whether further image augmentation or preprocessing is necessary to improve model generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9d852bc"
      },
      "outputs": [],
      "source": [
        "# Initialize data loader\n",
        "data_loader = CAPTCHADataLoader(DATA_DIR, IMG_HEIGHT, IMG_WIDTH)\n",
        "# Gets 50 000 images from dataset\n",
        "image_paths, labels = data_loader.get_data(limit=50000)\n",
        "\n",
        "print(f\"Total samples: {len(image_paths)}\")\n",
        "print(f\"Label examples: {labels[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbdc9fc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculates character frequency distribution\n",
        "char_freq = Counter(\"\".join(labels))\n",
        "\n",
        "if char_freq:\n",
        "    sorted_chars = char_freq.most_common()\n",
        "    top_chars = sorted_chars[:20]\n",
        "\n",
        "    if top_chars:  \n",
        "        fig, ax = plt.subplots(figsize=(12, 5))\n",
        "        chars, freqs = zip(*top_chars)\n",
        "        ax.bar(range(len(chars)), freqs, color='#F18F01', edgecolor='black', alpha=0.8)\n",
        "        ax.set_xticks(range(len(chars)))\n",
        "        ax.set_xticklabels(chars, rotation=45, fontsize=10)\n",
        "        ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Character', fontsize=11, fontweight='bold')\n",
        "        ax.set_title('Top 20 Character Frequency Distribution', fontsize=13, fontweight='bold')\n",
        "        ax.grid(alpha=0.3, axis='y')\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for i, freq in enumerate(freqs):\n",
        "            ax.text(i, freq + max(freqs)*0.01, str(freq), \n",
        "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(FIGURE_DIR, 'character_frequency.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"Total unique characters: {len(char_freq)}\")\n",
        "        print(f\"Most common character: '{sorted_chars[0][0]}' ({sorted_chars[0][1]} occurrences)\")\n",
        "    else:\n",
        "        print(\"No character frequency data to display\")\n",
        "else:\n",
        "    print(\"No characters found in labels - check if data was loaded correctly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eda_sample_images"
      },
      "outputs": [],
      "source": [
        "# Displays sample  CAPTCHA images\n",
        "fig, axes = plt.subplots(3, 4, figsize=(15, 10))\n",
        "# Selects 12 images from dataset\n",
        "sample_indices = np.random.choice(len(image_paths), 12, replace=False)\n",
        "\n",
        "for idx, ax in enumerate(axes.flatten()):\n",
        "    img_path = image_paths[sample_indices[idx]]\n",
        "    label = labels[sample_indices[idx]]\n",
        "    \n",
        "    img = Image.open(img_path)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"Label: {label}\", fontsize=10, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Sample CAPTCHA Images from Dataset', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURE_DIR, 'eda_sample_images.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eda_dimensions"
      },
      "outputs": [],
      "source": [
        "# Analyze image dimensions and color characteristics\n",
        "dimensions = []\n",
        "channels_count = []\n",
        "\n",
        "# Sample subset of images for efficiency\n",
        "sample_size = min(1000, len(image_paths))\n",
        "sample_paths = np.random.choice(image_paths, sample_size, replace=False)\n",
        "successful_reads = 0\n",
        "\n",
        "for img_path in tqdm(sample_paths):\n",
        "    try:\n",
        "        with Image.open(img_path) as img:\n",
        "            dimensions.append(img.size)\n",
        "            channels_count.append(len(img.getbands()))\n",
        "            successful_reads += 1\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "if successful_reads > 0:\n",
        "    dimensions = np.array(dimensions)\n",
        "    print(f\"Color Channels:\")\n",
        "    print(f\"  Unique channel counts: {np.unique(channels_count)}\")\n",
        "    print(f\"  Most common: {np.bincount(channels_count).argmax()} channels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eda_pixel_analysis"
      },
      "outputs": [],
      "source": [
        "# Analyze pixel intensity distributions\n",
        "pixel_intensities = {'Red': [], 'Green': [], 'Blue': []}\n",
        "sample_size = min(500, len(image_paths))\n",
        "sample_paths = np.random.choice(image_paths, sample_size, replace=False)\n",
        "\n",
        "successful_reads = 0\n",
        "\n",
        "for img_path in tqdm(sample_paths):\n",
        "    try:\n",
        "        with Image.open(img_path) as img:\n",
        "            img_rgb = img.convert('RGB')\n",
        "            img_array = np.array(img_rgb)\n",
        "            pixel_intensities['Red'].extend(img_array[:, :, 0].flatten())\n",
        "            pixel_intensities['Green'].extend(img_array[:, :, 1].flatten())\n",
        "            pixel_intensities['Blue'].extend(img_array[:, :, 2].flatten())\n",
        "            successful_reads += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        pass\n",
        "\n",
        "print(f\"Successfully processed {successful_reads}/{sample_size} images\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "colors = ['#E63946', '#06A77D', '#0077B6']\n",
        "channel_names = ['Red', 'Green', 'Blue']\n",
        "\n",
        "for idx, (channel, color) in enumerate(zip(channel_names, colors)):\n",
        "    intensities = np.array(pixel_intensities[channel])\n",
        "    if len(intensities) > 0:\n",
        "        axes[idx].hist(intensities, bins=50, color=color, edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_xlabel('Pixel Intensity', fontsize=11, fontweight='bold')\n",
        "        axes[idx].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "        axes[idx].set_title(f'{channel} Channel Distribution', fontsize=12, fontweight='bold')\n",
        "        axes[idx].grid(alpha=0.3, axis='y')\n",
        "        \n",
        "        print(f\"\\n{channel} Channel Statistics:\")\n",
        "        print(f\"  Mean: {intensities.mean():.2f}\")\n",
        "        print(f\"  Std: {intensities.std():.2f}\")\n",
        "        print(f\"  Min: {intensities.min()}\")\n",
        "        print(f\"  Max: {intensities.max()}\")\n",
        "    else:\n",
        "        axes[idx].text(0.5, 0.5, 'No Data', ha='center', va='center', transform=axes[idx].transAxes)\n",
        "        axes[idx].set_title(f'{channel} Channel (No Data)', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Pixel Intensity Distributions Across Color Channels', fontsize=13, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURE_DIR, 'eda_pixel_intensity.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f1aef97"
      },
      "source": [
        "# Train-Test-Validation Split\n",
        "\n",
        "The dataset is divided randomly into training, validation, and testing subsets containing 70%, 10%, and 20% of the data, respectively, while ensuring balanced label length distributions across the splits. A 70% `training set` offers a decent number of examples for the model to learn different patterns, features, and distortions in CAPTCHA images. This helps ensure its generalization over different character sequences, colors, and various distortions that might commonly be seen in CAPTCHAs.\n",
        "\n",
        "The `validation set`, representing 10% of the data, works as an independent set that monitors the model's performance during training. It enables the tuning of hyperparameters, the evaluation of overfitting, and the assessment of training stability without exposing the model to the test set. Thus, using a separate validation subset allows controlled guidance and optimization of the model's learning process.\n",
        "\n",
        "The `test set`, containing 20% of the data, is kept for final evaluation and hence provides an unbiased measure of the model's generalization ability to unseen CAPTCHAs. This split ensures that evaluation metrics reflect real-world performance and helps identify potential weaknesses or biases in the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d969b67"
      },
      "outputs": [],
      "source": [
        "# Splits data into subsets\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    image_paths, labels,\n",
        "    test_size=TEST_SPLIT,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=None\n",
        ")\n",
        "\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    train_paths, train_labels,\n",
        "    test_size=VAL_SPLIT,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=None\n",
        ")\n",
        "\n",
        "print(f\"Dataset split:\")\n",
        "print(f\"  Training samples: {len(train_paths)}\")\n",
        "print(f\"  Validation samples: {len(val_paths)}\")\n",
        "print(f\"  Test samples: {len(test_paths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f1aef97"
      },
      "source": [
        "# Dataset Preparation & Creation\n",
        "\n",
        "This section provides a high-performance data loading framework to construct efficient data pipelines from the training, validation, and test subsets. It generates each dataset with batch processing and optional shuffling to optimize memory usage and training speed. The training dataset is shuffled and augmented to improve model generalization, while the validation and test datasets are left in their original order to ensure that their evaluation is consistent.\n",
        "\n",
        "Batch size is set to the predefined configuration, enabling the model to handle several images at once, both during training and evaluation. The data pipeline also employs automatic performance tuning to optimize the prefetching, parallel processing, and loading of data. These pipelines perform on-the-fly image preprocessing, normalization, and label encoding; these steps ensure that all images are properly formatted and ready for input into the convolutional neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9fc784a"
      },
      "outputs": [],
      "source": [
        "train_dataset = create_dataset(\n",
        "    train_paths, \n",
        "    train_labels, \n",
        "    data_loader,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "val_dataset = create_dataset(\n",
        "    val_paths,\n",
        "    val_labels,\n",
        "    data_loader,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "test_dataset = create_dataset(\n",
        "    test_paths,\n",
        "    test_labels,\n",
        "    data_loader,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    is_training=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b29bea03"
      },
      "source": [
        "# Convolutional Neural Network Architecture for CAPTCHA Recognition\n",
        "\n",
        "This is a deep convolutional neural network design targeted at the recognition of CAPTCHAs, which are usually sets of distorted, rotated, or overlapping characters. CAPTCHA-solving CNNs require architectures able to extract both local and global features, capture sequential dependencies, and remain robust against noise, distortion, and variability across positions of characters. Each design choice within this network has been made to help address one of these main challenges.\n",
        "\n",
        "**Convolutional Blocks and Kernel Sizes**  \n",
        "It starts with a number of convolutional blocks. The first block uses large 7 × 7 kernels, which were employed to capture a broad, low-level feature encompassing edges, strokes, and basic character shapes (Krizhevsky, Sutskever and Hinton, 2012). These features are critical for distinguishing between visually similar characters in the distorted CAPTCHA images. Larger kernels provide a larger spatial context for early stages, which is important for the initial recognition of character structures that may be partially occluded or warped (Goodfellow, Bengio and Courville, 2016). The next blocks use smaller kernels of size 5 × 5 and 3 × 3 with increasing filter counts to capture mediumand fine-grained features. These layers focus on the subtle difference among different characters, small local distortions, and patterns that consistently occur over multiple positions of characters (Rawat and Wang, 2017). Stacking more convolutional layers with smaller kernels allows the network to learn hierarchical feature representations by gradually combining simple features into more complex ones specific to characters and sequences (Lecun, Bengio and Hinton, 2015).\n",
        "\n",
        "**Batch Normalization**  \n",
        "Batch normalization is applied after nearly every convolutional and dense layer; this stabilizes the process of learning by normalizing the activations, reducing internal covariate shift, and allowing higher learning rates (Goodfellow, Bengio and Courville, 2016). This technique also brings a regularization effect that enables the network to generalize well on previously unseen CAPTCHAs, usually containing a wide variety of distortions and noise patterns (Rawat and Wang, 2017).\n",
        "\n",
        "**Pooling Layers and Feature Consolidation**  \n",
        "Each convolutional block is followed by max pooling layers that decrease the spatial dimensions while retaining the most salient features (Rawat and Wang, 2017). Pooling consolidates information, makes feature maps more robust to minor spatial variations, and reduces computational complexity. The last convolutional block is followed by a global average pooling layer that converts the spatial feature maps into a compact feature vector representative of the overall content of the CAPTCHA image. This pooling operation preserves essential spatial information while significantly reducing the number of parameters, which is important to prevent overfitting given the high variability in CAPTCHA datasets (Lecun, Bengio and Hinton, 2015).\n",
        "\n",
        "**Dropout and Regularization**  \n",
        "Dropout is applied at numerous points in the network; the deeper the layer, the higher the dropout rate - ranging from 0.2 to 0.5 for convolutional blocks and from 0.3 to 0.4 for fully connected layers. This acts as regularization for the network, insuring that it will not quickly converge to overfit on one particular feature (Srivastava et al., 2014). The higher dropout in deeper layers balances the higher capacity of deeper feature representations against the risk of overfitting, which is very important while processing rather complex, high-dimensional CAPTCHA images (Rawat and Wang, 2017).\n",
        "\n",
        "**Multi-Branch Fully Connected Layers for Character Prediction**  \n",
        "After feature extraction, the network splits into independent, fully connected layers for every character position in the CAPTCHA. Each branch consists of two dense layers with batch normalization and dropout, followed by a softmax output layer predicting the probability distribution over all possible characters (Lecun, Bengio and Hinton, 2015). The presence of several branches is the characteristic of CAPTCHA-solving CNNs; it allows the network to make independent predictions over characters while they share the same convolutional feature representation. This avoids using recurrent layers while preserving sequence awareness, hence effectively handling CAPTCHAs of different character lengths and positions (Rawat and Wang, 2017).\n",
        "\n",
        "**Design Rationale for CAPTCHA Solving**  \n",
        "- Large initial kernels capture broad character structures, aiding early feature detection (Krizhevsky, Sutskever and Hinton, 2012). \n",
        "- Smaller kernels in deeper blocks allow the recognition of fine-grained patterns, which are critical in distinguishing similar characters (Rawat and Wang, 2017). \n",
        "- Increasing the filter counts allows the network to learn higher-level diverse feature representations (Lecun, Bengio and Hinton, 2015). \n",
        "- Batch normalization stabilizes training and speeds up convergence. It also regularizes the network (Goodfellow, Bengio and Courville, 2016). \n",
        "- Max and global average pooling consolidate spatial information while reducing the model parameters, hence improving generalization (Lecun, Bengio and Hinton, 2015).\n",
        "- Dropout across convolutional and fully-connected layers helps mitigate overfitting from noisy and distorted CAPTCHA data (Srivastava et al., 2014). \n",
        "- The multi-branch output structure allows independent character predictions to be made while leveraging shared features, thus balancing efficiency and accuracy in sequence recognition (Rawat and Wang, 2017)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6666bb2b"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model(img_height, img_width, num_characters, max_label_length):\n",
        "    input_img = layers.Input(shape=(img_height, img_width, 3), name='image_input')\n",
        "    \n",
        "    # Initial convolution to capture character-level features\n",
        "    x = layers.Conv2D(64, (7, 7), activation='relu', padding='same')(input_img)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(64, (7, 7), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x) \n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Second block - optimized for horizontal features\n",
        "    x = layers.Conv2D(128, (5, 5), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(128, (5, 5), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Third block - character detail extraction\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x) \n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Fourth block - high-level feature consolidation\n",
        "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.35)(x)\n",
        "\n",
        "    # Final feature consolidation\n",
        "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    \n",
        "    # Separate branches per character\n",
        "    outputs = []\n",
        "    for i in range(max_label_length):\n",
        "        char_branch = layers.Dense(512, activation='relu', name=f'char_{i+1}_dense1')(x)\n",
        "        char_branch = layers.BatchNormalization(name=f'char_{i+1}_bn1')(char_branch)\n",
        "        char_branch = layers.Dropout(0.4)(char_branch)\n",
        "        char_branch = layers.Dense(256, activation='relu', name=f'char_{i+1}_dense2')(char_branch)\n",
        "        char_branch = layers.BatchNormalization(name=f'char_{i+1}_bn2')(char_branch)\n",
        "        char_branch = layers.Dropout(0.3)(char_branch)\n",
        "        char_output = layers.Dense(num_characters, activation='softmax', name=f'char_{i+1}')(char_branch)\n",
        "        outputs.append(char_output)\n",
        "\n",
        "    return models.Model(inputs=input_img, outputs=outputs, name='CAPTCHA_CNN_Solver')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53e0553a"
      },
      "source": [
        "# Initialization and Compilation of Model\n",
        "\n",
        "It deals with preparing the convolutional neural network for either training or evaluation. If a previously trained model exists at the given path, it is loaded to resume training or carry out the evaluation to save on computational resources. If there is no saved model, a new CNN model is created from the predefined architecture, which has already been designed for CAPTCHA recognition.\n",
        "\n",
        "This network is then compiled with separate categorical cross-entropy losses at each character position, given the multi-output nature of CAPTCHA-solving tasks. Each character branch offers an accuracy metric for detailed monitoring at each position in the CAPTCHA sequence. An adaptive optimizer updates network weights; an adjustable learning rate provides a trade-off between convergence speed and stability. This setup prepares the model for learning from the preprocessed CAPTCHA datasets, while appropriate metrics are available to measure the progress of learning. A summary of the model provides an overview of this architecture, such as number of parameters, layer types, and output branches, which is useful to verify the design of the network before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b35f47cc"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(METRIC_DIR, \"best_model.keras\")\n",
        "\n",
        "# Either loads existing model or builds new model\n",
        "if os.path.exists(model_path):\n",
        "    model = load_model(model_path)\n",
        "else:\n",
        "    model = build_cnn_model(\n",
        "        IMG_HEIGHT,\n",
        "        IMG_WIDTH,\n",
        "        len(data_loader.characters),\n",
        "        max_label_length=6\n",
        ")\n",
        "\n",
        "losses = {f'char_{i}': 'categorical_crossentropy' for i in range(1, 7)}\n",
        "metrics = {f'char_{i}': 'accuracy' for i in range(1, 7)}\n",
        "\n",
        "# Compiles model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=losses,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ebe2769"
      },
      "source": [
        "# Training Configuration and Metrics Logging\n",
        "\n",
        "This section details the callback mechanisms responsible for keeping track of, saving, and managing model performance throughout training. A custom callback has been created that will log per-epoch training and validation losses, as well as all metrics for each character prediction. Three separate JSON files are maintained: one for all epoch losses, one for the epoch with the best monitored metric, and one for comprehensive per-character metrics. The structured logging allows for significant analysis of model behavior over time and may provide persistent records for later reproducibility and evaluation.\n",
        "\n",
        "The process incorporates some extra callbacks to help convergence and avoid overfitting. Early stopping monitors the validation loss and stops training if there is no improvement for a given number of epochs, restoring the best weights to guarantee that the model is optimal. Model checkpointing saves the best model in terms of validation loss and keeps the most performant network. Learning rate reduction automatically decreases the learning rate when validation performance has plateaued, allowing finer adjustments in model weights, which can lead to better convergence.\n",
        "\n",
        "The model then trains on the prepared training dataset while validating on a separate validation dataset. Metrics logging, early stopping, checkpointing, and learning rate reduction run parallel, guiding this training process with real-time monitoring, automatic adjustments, and robust performance tracking of each character in CAPTCHA sequences. This configuration will ensure efficient and reliable training but also keep detailed records for analysis and model evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73cb63ac",
      "metadata": {},
      "source": [
        "## Metrics Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2040473",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MetricsLogger(Callback):\n",
        "    \"\"\"\n",
        "    Logs per-epoch training and validation loss.\n",
        "    - loss_metrics.json: both losses per epoch\n",
        "    - best_metrics.json: only epoch with best monitored metric \n",
        "    - all_metrics.json: all metrics every epoch\n",
        "    \"\"\"\n",
        "    def __init__(self, log_dir, monitor=\"val_loss\", mode=\"min\"):\n",
        "        super().__init__()\n",
        "        self.log_dir = log_dir\n",
        "        self.monitor = monitor\n",
        "        self.loss_metrics_path = os.path.join(log_dir, \"loss_metrics.json\")\n",
        "        self.best_metrics_path = os.path.join(log_dir, \"best_metrics.json\")\n",
        "        self.all_metrics_path = os.path.join(log_dir, \"all_metrics.json\")  \n",
        "        self.loss_metrics = []\n",
        "        self.all_metrics = []\n",
        "\n",
        "        # Determine comparison function based on mode\n",
        "        if mode == \"min\":\n",
        "            self.monitor_op = lambda a, b: a < b\n",
        "            self.best_value = float(\"inf\")\n",
        "        elif mode == \"max\":\n",
        "            self.monitor_op = lambda a, b: a > b\n",
        "            self.best_value = float(\"-inf\")\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'min' or 'max'\")\n",
        "\n",
        "        self.best_metrics = {}\n",
        "\n",
        "    # Function runs after every epoch\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        train_loss = logs.get(\"loss\", None)\n",
        "        val_loss = logs.get(\"val_loss\", None)\n",
        "        monitored_value = logs.get(self.monitor, None)\n",
        "\n",
        "        epoch_metrics = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"loss\": train_loss,\n",
        "            \"val_loss\": val_loss\n",
        "        }\n",
        "        self.loss_metrics.append(epoch_metrics)\n",
        "\n",
        "        with open(self.loss_metrics_path, \"w\") as f:\n",
        "            json.dump(self.loss_metrics, f, indent=4)\n",
        "\n",
        "        # Update best metrics if epochs metrics are new best\n",
        "        if monitored_value is not None and self.monitor_op(monitored_value, self.best_value):\n",
        "            self.best_value = monitored_value\n",
        "            self.best_metrics = epoch_metrics\n",
        "            with open(self.best_metrics_path, \"w\") as f:\n",
        "                json.dump(self.best_metrics, f, indent=4)\n",
        "\n",
        "        all_epoch_metrics = {\"epoch\": epoch + 1}\n",
        "        for key, value in logs.items():\n",
        "            all_epoch_metrics[key] = float(value) \n",
        "        self.all_metrics.append(all_epoch_metrics)\n",
        "\n",
        "        with open(self.all_metrics_path, \"w\") as f:\n",
        "            json.dump(self.all_metrics, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c23790",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b35f47cc"
      },
      "outputs": [],
      "source": [
        "# Metric logger initialization\n",
        "metrics_logger = MetricsLogger(\n",
        "    log_dir=METRIC_DIR,      \n",
        "    monitor=\"val_loss\",       \n",
        "    mode=\"min\"                 \n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=7,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        os.path.join(METRIC_DIR, 'best_model.keras'),\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-5,\n",
        "        verbose=1\n",
        "    ),\n",
        "    metrics_logger\n",
        "]\n",
        "\n",
        "# Trains Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "539656fc"
      },
      "source": [
        "# Model Evaluation and Performance Metrics\n",
        "\n",
        "This part covers the evaluation of the trained CNN model on both validation and test datasets and extracts and computes key performance metrics. Previously saved model weights and metrics are loaded, if available, for reproducibility and comparison across training sessions. `Validation loss` is calculated to assess model performance during training, while `test loss` measures generalization to unseen data.\n",
        "\n",
        "Predictions on the test dataset are generated and decoded to character sequences, allowing for comparison with true labels. One-hot encodings are used to reconstruct true labels, while predicted labels have their length aligned to correctly evaluate them. Full CAPTCHA accuracy is calculated as a proportion of test samples in which the entirety of the predicted sequence matches ground truth precisely, which correctly reflects the network's ability to solve full CAPTCHAs rather than individual characters. Character-level accuracy can be derived by averaging per-character metrics across the sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15af7753",
      "metadata": {},
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d0b756",
      "metadata": {},
      "outputs": [],
      "source": [
        "BEST_MODEL_PATH = os.path.join(METRIC_DIR, \"best_model.keras\")\n",
        "BEST_METRICS_PATH = os.path.join(METRIC_DIR, \"best_metrics.json\")\n",
        "LOSS_METRICS_PATH = os.path.join(METRIC_DIR, \"loss_metrics.json\")\n",
        "ALL_METRICS_PATH = os.path.join(METRIC_DIR, \"all_metrics.json\")\n",
        "\n",
        "# Loads best metrics\n",
        "if os.path.exists(BEST_METRICS_PATH):\n",
        "    with open(BEST_METRICS_PATH, \"r\") as f:\n",
        "        best_metrics = json.load(f)\n",
        "\n",
        "# Loads loss metrics\n",
        "if os.path.exists(LOSS_METRICS_PATH):\n",
        "    with open(LOSS_METRICS_PATH, \"r\") as f:\n",
        "        loss_metrics = json.load(f)\n",
        "\n",
        "all_metrics = \"\"\n",
        "\n",
        "# Loads all metrics\n",
        "if os.path.exists(ALL_METRICS_PATH):\n",
        "    with open(ALL_METRICS_PATH, \"r\") as f:\n",
        "        all_metrics = json.load(f)\n",
        "       \n",
        "if model is None and os.path.exists(BEST_MODEL_PATH):\n",
        "    model = tf.keras.models.load_model(BEST_MODEL_PATH)\n",
        "   \n",
        "# Evaluate the model on the validation dataset   \n",
        "val_results = model.evaluate(val_dataset, verbose=1)\n",
        "val_loss = val_results[0]\n",
        "        \n",
        "# Evaluate the model on the test dataset\n",
        "test_results = model.evaluate(test_dataset, verbose=1)\n",
        "test_loss = test_results[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c5eab2",
      "metadata": {},
      "source": [
        "## Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "505f44dd"
      },
      "outputs": [],
      "source": [
        "# Runs predictions\n",
        "raw_predictions = model.predict(test_dataset, verbose=1)\n",
        "predictions = np.stack(raw_predictions, axis=1) if isinstance(raw_predictions, list) else raw_predictions\n",
        "\n",
        "max_label_length = max(len(label) for label in labels)\n",
        "\n",
        "print(f\"\\nPredictions shape: {predictions.shape}\") \n",
        "\n",
        "# Extract true labels\n",
        "true_labels = []\n",
        "for images, label_dict in test_dataset:\n",
        "    batch_size = label_dict['char_1'].shape[0]\n",
        "    for i in range(batch_size):\n",
        "        char_indices = [int(np.argmax(label_dict[f'char_{pos+1}'][i].numpy()))\n",
        "                        for pos in range(max_label_length)]\n",
        "        decoded = ''.join(data_loader.num_to_char[idx] for idx in char_indices)\n",
        "        true_labels.append(decoded)\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Decode predicted labels\n",
        "predicted_labels = []\n",
        "num_samples = predictions.shape[0]\n",
        "for i in range(num_samples):\n",
        "    char_indices = np.argmax(predictions[i], axis=-1)\n",
        "    decoded = ''.join(data_loader.num_to_char[int(idx)] for idx in char_indices)\n",
        "    predicted_labels.append(decoded)\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "# Align lengths if mismatch\n",
        "if len(predicted_labels) != len(true_labels):\n",
        "    print(f\"Warning: Pred/true mismatch {len(predicted_labels)} vs {len(true_labels)}\")\n",
        "    m = min(len(predicted_labels), len(true_labels))\n",
        "    predicted_labels = predicted_labels[:m]\n",
        "    true_labels = true_labels[:m]\n",
        "\n",
        "# Compute CAPTCHA accuracy\n",
        "absolute_correct = np.sum(predicted_labels == true_labels)\n",
        "final_accuracy = absolute_correct / len(true_labels)\n",
        "\n",
        "accuracy_metrics = [value for name, value in zip(model.metrics_names, test_results) if \"accuracy\" in name.lower() and not np.isnan(value)] \n",
        "avg_character_accuracy = float(np.mean(accuracy_metrics))\n",
        "\n",
        "print(f\" CAPTCHA solving Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\" Correct: {absolute_correct}/{len(true_labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efa76047"
      },
      "source": [
        "# Performance Visualization and Detailed Metrics Analysis\n",
        "\n",
        "This section visualizes and analyzes the final performance of the model using various complementary approaches. Final performance metrics such as `final_accuracy`, `test-loss`, and `val_loss` are presented in a color-coded bar chart, allowing for immediate comparison of key outcomes. Loss curves for training and validation across epochs are plotted to gauge convergence stability and eventual trends related to overfitting or underfitting during training. Average character-level accuracy is calculated per epoch, a fine-grained look at the model's learning dynamics for specific characters in the CAPTCHA sequences.\n",
        "\n",
        "Confusion matrices are created at both the overall and per-character-position levels, allowing for the identification of systematic errors in character prediction and positions most prone to misclassification. This gives an indication of which specific characters or CAPTCHA positions the model struggles with and informs possible future improvements in either preprocessing or adjusting model architecture. Correct and incorrect predictions are visualized with sample images; the characters are highlighted using color coding to indicate accuracy, enabling qualitative inspection of typical failure modes versus successful predictions.\n",
        "\n",
        "Per-character accuracy across the positions of CAPTCHA is calculated and visualized using a gradient-colored bar chart that shows which positions have higher or lower prediction reliability. Statistical summaries emphasize the strongest and weakest positions within the sequence, targeting the refinement of the network. ROC and Precision–Recall curves are created from flattened one-hot true labels and predicted probabilities, which quantify the model's discriminative capability and the balance of precision-recall across thresholds. These visualizations, combined with metrics including `AUC` and `average_precision`, provide a comprehensive evaluation of the effectiveness of the CNN in solving CAPTCHA tasks and its ability to correctly identify characters under complex distortions and noise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac39362c",
      "metadata": {},
      "source": [
        "## Final Performance Metrics\n",
        "\n",
        "The **Final Performance Metrics** section visualizes the model's key quantitative outcomes: `final_accuracy`, `test_loss`, and `val_loss`. Each metric is shown in a color-coded bar chart with values annotated, making it easier to see at a glance the general model behavior, as well as the balance between accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8318fc0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metric arrays\n",
        "metrics_names = ['CAPTCHA Accuracy', 'Test Loss', 'Validation Loss']\n",
        "metrics_values = [final_accuracy, test_loss, val_loss]\n",
        "metrics_colors = ['#06A77D', '#E63946', '#F18F01']\n",
        "\n",
        "metrics_values = [float(np.atleast_1d(v)[0]) for v in metrics_values]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "bars = plt.bar(metrics_names, metrics_values, color=metrics_colors, edgecolor='black', alpha=0.7)\n",
        "plt.ylabel('Value')\n",
        "plt.title('Final Performance Metrics')\n",
        "\n",
        "plt.ylim([0, max(metrics_values)*1.1 if metrics_values else 1])\n",
        "\n",
        "# Annotate bars\n",
        "for bar, val in zip(bars, metrics_values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., val + 0.01, f'{val:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURE_DIR, 'final_metrics.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7c6a31",
      "metadata": {},
      "source": [
        "## Training and Validation Curves & Average Character Accuracy\n",
        "\n",
        "The learning dynamics are evaluated by plotting the **Training and Validation Loss** over epochs. Such curves allow for the identification of underfitting, overfitting, or stable convergence patterns. Overlaying the `val_loss` gives an insight into the generalization, whereas the `loss` is indicative of how well the model learns to capture the underlying patterns in the training data.\n",
        "\n",
        "The **Average Character Accuracy Over Epochs** gives a more fine-grained insight into the model's performance on the single character level. Averaging the accuracies across all positions shows the learning trend for each character and helps in identifying those positions which are consistently more difficult to predict due to either the structural or visual complexity of CAPTCHAs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9aed32f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Loss curves\n",
        "if history.history and len(history.history.get('loss', [])) > 0:\n",
        "    epochs_range = range(1, len(history.history['loss']) + 1)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(epochs_range, history.history['loss'], label=\"Training Loss\", color=\"#2E86AB\", linewidth=2)\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(epochs_range, history.history['val_loss'], label=\"Validation Loss\", color=\"#A23B72\", linewidth=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIGURE_DIR, 'loss_curves.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Average Character Accuracy from history\n",
        "    train_acc_keys = [k for k in history.history.keys() if k.endswith('_accuracy') and not k.startswith('val_')]\n",
        "    val_acc_keys = [k for k in history.history.keys() if k.startswith('val_') and k.endswith('_accuracy')]\n",
        "\n",
        "    if train_acc_keys:\n",
        "        train_acc_arrays = [history.history[k] for k in train_acc_keys]\n",
        "        avg_train_acc = np.mean(train_acc_arrays, axis=0)\n",
        "        avg_character_accuracy = avg_train_acc[-1]\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(epochs_range, avg_train_acc, label=\"Training Accuracy\", color=\"#2E86AB\", linewidth=2)\n",
        "\n",
        "        if val_acc_keys:\n",
        "            val_acc_arrays = [history.history[k] for k in val_acc_keys]\n",
        "            avg_val_acc = np.mean(val_acc_arrays, axis=0)\n",
        "            plt.plot(epochs_range, avg_val_acc, label=\"Validation Accuracy\", color=\"#A23B72\", linewidth=2)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Average Character Accuracy\")\n",
        "        plt.title(\"Average Character Accuracy Over Epochs\")\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(FIGURE_DIR, 'avg_char_accuracy.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "# Version if history.history is empty\n",
        "elif loss_metrics:\n",
        "    epochs_range = range(1, len(loss_metrics) + 1)\n",
        "\n",
        "    # gets Training and validation loss from Loss Metrics \n",
        "    train_loss = [m.get(\"loss\", None) for m in loss_metrics]\n",
        "    val_loss = [m.get(\"val_loss\", None) for m in loss_metrics]\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(epochs_range, train_loss, label=\"Training Loss\", color=\"#2E86AB\", linewidth=2)\n",
        "    if any(v is not None for v in val_loss):\n",
        "        plt.plot(epochs_range, val_loss, label=\"Validation Loss\", color=\"#A23B72\", linewidth=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIGURE_DIR, 'loss_curves.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Average Character Accuracy using all_metrics\n",
        "    if all_metrics:\n",
        "        avg_char_acc_per_epoch = []\n",
        "\n",
        "        for epoch_metrics in all_metrics:\n",
        "            char_acc_values = [v for k, v in epoch_metrics.items() if k.endswith('_accuracy') and k.startswith('char_')]\n",
        "            if char_acc_values:\n",
        "                avg_char_acc_per_epoch.append(np.mean(char_acc_values))\n",
        "            else:\n",
        "                avg_char_acc_per_epoch.append(None)\n",
        "\n",
        "        # Plot Average Character Accuracy\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(epochs_range, avg_char_acc_per_epoch, label=\"Average Character Accuracy\", color=\"#2E86AB\", linewidth=2)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Average Character Accuracy\")\n",
        "        plt.title(\"Average Character Accuracy Over Epochs\")\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(FIGURE_DIR, 'avg_char_accuracy.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ca7915",
      "metadata": {},
      "source": [
        "## Confusion Matrices\n",
        "\n",
        "The **Overall Character-Level Confusion Matrix** aggregates the predictions across all positions to indicate systemic misclassification patterns, while the **Character-Level Confusion Matrix for Each Position** isolates performance for each position of the CAPTCHA sequence. These matrices are displayed as heatmaps, where colour intensities show the counts of misclassifications and tick labels correspond to actual characters, enabling the detection of biases or frequently confused characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5812ed1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_true = []\n",
        "all_pred = []\n",
        "\n",
        "for i, label in enumerate(true_labels):\n",
        "    for pos, true_char in enumerate(label):\n",
        "        if pos >= predictions.shape[1]:\n",
        "            continue\n",
        "        true_idx = data_loader.char_to_num.get(true_char, -1)\n",
        "        if true_idx == -1:\n",
        "            continue\n",
        "        pred_idx = np.argmax(predictions[i, pos, :])\n",
        "        all_true.append(true_idx)\n",
        "        all_pred.append(pred_idx)\n",
        "\n",
        "all_true = np.array(all_true)\n",
        "all_pred = np.array(all_pred)\n",
        "\n",
        "cm = confusion_matrix(all_true, all_pred, labels=list(range(len(data_loader.characters))))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 12))\n",
        "sns.heatmap(cm, annot=False, cmap='YlOrRd', cbar_kws={'label': 'Misclassification Count'}, ax=ax)\n",
        "ax.set_title('Overall Character-Level Confusion Matrix')\n",
        "ax.set_xlabel('Predicted Character')\n",
        "ax.set_ylabel('True Character')\n",
        "tick_labels = [data_loader.num_to_char[i] for i in range(len(data_loader.characters))]\n",
        "ax.set_xticks(np.arange(len(tick_labels)) + 0.5)\n",
        "ax.set_yticks(np.arange(len(tick_labels)) + 0.5)\n",
        "ax.set_xticklabels(tick_labels, rotation=45)\n",
        "ax.set_yticklabels(tick_labels, rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURE_DIR, 'confusion_matrix_overall.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fc09b7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop over each character position\n",
        "for pos in range(max_label_length):\n",
        "    if predictions.ndim == 3 and predictions.shape[1] > pos:\n",
        "        # Extract predictions for the current position\n",
        "        char_preds = predictions[:, pos, :]\n",
        "        char_predicted = np.argmax(char_preds, axis=1)\n",
        "        \n",
        "        # Extract true labels for the current character position\n",
        "        char_true = []\n",
        "        for label in test_labels:\n",
        "            if len(label) > pos:\n",
        "                char = label[pos]\n",
        "                if char in data_loader.char_to_num:\n",
        "                    char_true.append(data_loader.char_to_num[char])\n",
        "                else:\n",
        "                    char_true.append(-1)  # handle unknown characters\n",
        "            else:\n",
        "                char_true.append(-1)\n",
        "        \n",
        "        # Filter out invalid labels\n",
        "        valid_indices = [i for i, true_val in enumerate(char_true) if true_val != -1]\n",
        "        if valid_indices:\n",
        "            char_true_valid = [char_true[i] for i in valid_indices]\n",
        "            char_predicted_valid = [char_predicted[i] for i in valid_indices]\n",
        "            \n",
        "            # Compute confusion matrix\n",
        "            cm = confusion_matrix(\n",
        "                char_true_valid, \n",
        "                char_predicted_valid, \n",
        "                labels=list(range(len(data_loader.characters)))\n",
        "            )\n",
        "            \n",
        "            # Plot heatmap\n",
        "            fig, ax = plt.subplots(figsize=(14, 12))\n",
        "            sns.heatmap(cm, annot=False, cmap='YlOrRd', cbar_kws={'label': 'Misclassification Count'}, ax=ax)\n",
        "            ax.set_title(f'Character-Level Confusion Matrix (Position {pos + 1})', fontsize=13, fontweight='bold', pad=20)\n",
        "            ax.set_ylabel('True Character', fontsize=11, fontweight='bold')\n",
        "            ax.set_xlabel('Predicted Character', fontsize=11, fontweight='bold')\n",
        "            # Set tick labels to actual characters\n",
        "            tick_labels = [data_loader.num_to_char.get(i, '') for i in range(len(data_loader.characters))]\n",
        "            ax.set_xticks(np.arange(len(tick_labels)) + 0.5)\n",
        "            ax.set_yticks(np.arange(len(tick_labels)) + 0.5)\n",
        "            ax.set_xticklabels(tick_labels, rotation=45)\n",
        "            ax.set_yticklabels(tick_labels, rotation=0)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(FIGURE_DIR, f'confusion_matrix_position_{pos + 1}.png'), dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "            accuracy = np.trace(cm) / np.sum(cm)\n",
        "            print(f\"Accuracy for character position {pos + 1}: {accuracy:.4f}\")\n",
        "        else:\n",
        "            print(f\"No valid labels found for confusion matrix at position {pos + 1}\")\n",
        "    else:\n",
        "        print(f\"Invalid predictions shape for confusion matrix at position {pos + 1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aaacfd8",
      "metadata": {},
      "source": [
        "## Error and Correct Prediction Image Analysis with Per-Character Highlighting\n",
        "\n",
        "**Sample Misclassified CAPTCHA Images with Character Highlighting** provides a qualitative take on model errors. Incorrectly predicted characters are highlighted in red underneath the images, by which one can identify common types of error, such as misidentified or skipped characters. The same goes for **Sample Correctly Classified CAPTCHA Images with Character Highlighting**, which further helps solidify successful predictions through correctly identified characters displayed in green."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a69b1c1",
      "metadata": {
        "id": "error_analysis"
      },
      "outputs": [],
      "source": [
        "# Identify correct and incorrect predictions\n",
        "correct_indices = [i for i, (true, pred) in enumerate(zip(test_labels, predicted_labels)) if true == pred]\n",
        "incorrect_indices = [i for i, (true, pred) in enumerate(zip(test_labels, predicted_labels)) if true != pred]\n",
        "\n",
        "print(f\"Correct predictions: {len(correct_indices)} ({len(correct_indices)/len(test_labels)*100:.2f}%)\")\n",
        "print(f\"Incorrect predictions: {len(incorrect_indices)} ({len(incorrect_indices)/len(test_labels)*100:.2f}%)\")\n",
        "\n",
        "# Per-character errors for incorrect CAPTCHAs\n",
        "if incorrect_indices:\n",
        "    error_lengths = []\n",
        "    for idx in incorrect_indices:\n",
        "        true_label = test_labels[idx]\n",
        "        pred_label = predicted_labels[idx]\n",
        "        min_len = min(len(true_label), len(pred_label))\n",
        "        errors = sum(1 for i in range(min_len) if true_label[i] != pred_label[i])\n",
        "        error_lengths.append(errors)\n",
        "    \n",
        "    avg_errors = np.mean(error_lengths)\n",
        "    most_common_errors = max(set(error_lengths), key=error_lengths.count)\n",
        "    print(f\"Average wrong characters per incorrect CAPTCHA: {avg_errors:.2f}\")\n",
        "    print(f\"Most common error count: {most_common_errors}\")\n",
        "\n",
        "# Display incorrect predictions with per-character highlighting\n",
        "if incorrect_indices:\n",
        "    if 'test_paths' not in locals() and 'test_paths' not in globals():\n",
        "        print(\"Warning: test_paths not found. Cannot display misclassified images.\")\n",
        "    else:\n",
        "        fig, axes = plt.subplots(3, 4, figsize=(15, 10))\n",
        "        np.random.seed(42)\n",
        "        sample_incorrect = np.random.choice(incorrect_indices, min(12, len(incorrect_indices)), replace=False)\n",
        "        \n",
        "        for idx, ax in enumerate(axes.flatten()):\n",
        "            if idx < len(sample_incorrect):\n",
        "                img_idx = sample_incorrect[idx]\n",
        "                img_path = test_paths[img_idx]\n",
        "                true_label = test_labels[img_idx]\n",
        "                pred_label = predicted_labels[img_idx]\n",
        "                \n",
        "                try:\n",
        "                    img = Image.open(img_path).convert(\"RGB\")\n",
        "                    ax.imshow(img)\n",
        "                    ax.axis('off')\n",
        "\n",
        "                    # Highlight incorrect characters\n",
        "                    for i, (t, p) in enumerate(zip(true_label, pred_label)):\n",
        "                        color = 'red' if t != p else 'green'\n",
        "                        ax.text(\n",
        "                            (i + 0.5) * img.width / len(true_label),  # x-position\n",
        "                            img.height + 5,                           # y-position\n",
        "                            p, color=color, fontsize=12, ha='center', va='top', fontweight='bold'\n",
        "                        )\n",
        "\n",
        "                    ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=9, fontweight='bold')\n",
        "                except Exception as e:\n",
        "                    ax.text(0.5, 0.5, f\"Error loading image\\n{img_path}\", ha='center', va='center', transform=ax.transAxes)\n",
        "                    ax.axis('off')\n",
        "            else:\n",
        "                ax.axis('off')\n",
        "        \n",
        "        plt.suptitle('Sample Misclassified CAPTCHA Images with Character Highlighting', fontsize=13, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(FIGURE_DIR, 'misclassified_highlighted.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Display correct predictions with per-character highlighting \n",
        "if correct_indices:\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(15, 6))\n",
        "    np.random.seed(42)\n",
        "    sample_correct = np.random.choice(correct_indices, min(8, len(correct_indices)), replace=False)\n",
        "    \n",
        "    for idx, ax in enumerate(axes.flatten()):\n",
        "        if idx < len(sample_correct):\n",
        "            img_idx = sample_correct[idx]\n",
        "            img_path = test_paths[img_idx]\n",
        "            true_label = test_labels[img_idx]\n",
        "            pred_label = predicted_labels[img_idx]\n",
        "            \n",
        "            try:\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                ax.imshow(img)\n",
        "                ax.axis('off')\n",
        "\n",
        "                # Highlight all characters in green\n",
        "                for i, (t, p) in enumerate(zip(true_label, pred_label)):\n",
        "                    ax.text(\n",
        "                        (i + 0.5) * img.width / len(true_label),\n",
        "                        img.height + 5,\n",
        "                        p, color='green', fontsize=12, ha='center', va='top', fontweight='bold'\n",
        "                    )\n",
        "\n",
        "                ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=9, fontweight='bold', color='green')\n",
        "            except Exception as e:\n",
        "                ax.text(0.5, 0.5, f\"Error loading image\", ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.axis('off')\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.suptitle('Sample Correctly Classified CAPTCHA Images with Character Highlighting', fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIGURE_DIR, 'correct_highlighted.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b5104a",
      "metadata": {},
      "source": [
        "## Per-character Prediction Accuracy \n",
        "\n",
        "**Per-Position Character Prediction Accuracy** represents the accuracy for a character at each position in the CAPTCHA. The results to be reported include average accuracy, the strongest and weakest positions, and showing the findings in a gradient-colored bar chart that highlights the relative performance differences across the CAPTCHA sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "per_char_analysis"
      },
      "outputs": [],
      "source": [
        "# Per-character prediction accuracy\n",
        "char_accuracies = {}\n",
        "char_counts = {} \n",
        "\n",
        "for position in range(max_label_length):\n",
        "    correct_at_pos = 0\n",
        "    total_at_pos = 0\n",
        "    \n",
        "    for true_label, pred_label in zip(test_labels, predicted_labels):\n",
        "        # Check if both labels have at least position+1 characters\n",
        "        if position < len(true_label) and position < len(pred_label):\n",
        "            if true_label[position] == pred_label[position]:\n",
        "                correct_at_pos += 1\n",
        "            total_at_pos += 1\n",
        "        elif position < len(true_label) and position >= len(pred_label):\n",
        "            # True label has character at this position but prediction doesn't\n",
        "            total_at_pos += 1\n",
        "        elif position >= len(true_label) and position < len(pred_label):\n",
        "            # Prediction has character at this position but true label doesn't\n",
        "            total_at_pos += 1\n",
        "    \n",
        "    if total_at_pos > 0:\n",
        "        accuracy = correct_at_pos / total_at_pos\n",
        "        char_accuracies[f'Character {position+1}'] = accuracy\n",
        "        char_counts[f'Character {position+1}'] = total_at_pos\n",
        "    else:\n",
        "        char_accuracies[f'Character {position+1}'] = 0.0\n",
        "        char_counts[f'Character {position+1}'] = 0\n",
        "\n",
        "if char_accuracies:\n",
        "    fig, ax = plt.subplots(figsize=(12, 5))\n",
        "    positions = list(char_accuracies.keys())\n",
        "    accuracies = list(char_accuracies.values())\n",
        "    \n",
        "    # Create color gradient (darker green for higher accuracy)\n",
        "    colors = [f'#006D32' if acc >= 0.7 else f'#06A77D' if acc >= 0.5 else '#99D8C9' for acc in accuracies]\n",
        "    \n",
        "    bars = ax.bar(range(len(positions)), accuracies, color=colors, edgecolor='black', alpha=0.8)\n",
        "    ax.set_xticks(range(len(positions)))\n",
        "    ax.set_xticklabels(positions, rotation=45, fontsize=10)\n",
        "    ax.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
        "    ax.set_xlabel('Character Position in CAPTCHA', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('Per-Position Character Prediction Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylim([0, 1.5])\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels with percentage\n",
        "    for bar, acc, count in zip(bars, accuracies, char_counts.values()):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "               f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIGURE_DIR, 'per_position_accuracy.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    avg_position_accuracy = np.mean(accuracies)\n",
        "    weakest_position = positions[np.argmin(accuracies)]\n",
        "    strongest_position = positions[np.argmax(accuracies)]\n",
        "    \n",
        "    print(f\"Average position accuracy: {avg_position_accuracy:.4f}\")\n",
        "    print(f\"Strongest position: {strongest_position} ({np.max(accuracies):.4f})\")\n",
        "    print(f\"Weakest position: {weakest_position} ({np.min(accuracies):.4f})\")\n",
        "else:\n",
        "    print(\"No valid data for per-position accuracy analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af022fc7",
      "metadata": {},
      "source": [
        "## ROC Curve, AUC and Precision-Recall Curve\n",
        "\n",
        "Further analysis involves converting character sequences to integer-class arrays to accurately calculate metrics. Precision and recall are obtained for each character position, reflecting the model's ability to correctly detect characters under the challenging distortions normally faced in CAPTCHAs. One-hot encodings of true labels and predicted probabilities enable the construction of `ROC` and `Precision–Recall` curves, allowing the evaluation of model discriminative power and performance under a variety of thresholds. The metrics, such as `AUC` and `average precision`, provide quantitative measures of overall quality and reliability for the models dealing with CAPTCHA recognition.\n",
        "\n",
        "The **ROC Curve** and **Precision–Recall Curve** are used here to evaluate the discriminative capability of the classifier. ROC curves show the trade-off between true positive rate and false positive rate across all thresholds, while Precision–Recall curves emphasize performance in imbalanced scenarios. Calculated `AUC` and `average_precision` give numeric measures of robustness, showing the model's effectiveness in distinguishing between characters, even when the images are complex and distorted as in a typical CAPTCHA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89709fbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert true/pred labels → integer-class arrays\n",
        "y_true_int = np.array([\n",
        "    [data_loader.char_to_num[ch] for ch in captcha]\n",
        "    for captcha in true_labels\n",
        "])\n",
        "\n",
        "y_pred_int = np.array([\n",
        "    [data_loader.char_to_num[ch] for ch in captcha]\n",
        "    for captcha in predicted_labels\n",
        "])\n",
        "\n",
        "y_pred_probs = predictions\n",
        "num_classes = y_pred_probs.shape[-1]\n",
        "seq_len = y_pred_probs.shape[1]\n",
        "\n",
        "# Precision & Recall per character\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "\n",
        "for i in range(seq_len):\n",
        "    p = precision_score(y_true_int[:, i], y_pred_int[:, i], average='macro', zero_division=0)\n",
        "    r = recall_score(y_true_int[:, i], y_pred_int[:, i], average='macro', zero_division=0)\n",
        "    precision_scores.append(p)\n",
        "    recall_scores.append(r)\n",
        "\n",
        "# Build one-hot true y for ROC/PR curve\n",
        "y_true_1hot = np.eye(num_classes)[y_true_int]\n",
        "\n",
        "# Flatten true labels and predicted probas\n",
        "y_true_flat = y_true_1hot.ravel()\n",
        "y_pred_flat = y_pred_probs.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de20acd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_true_flat, y_pred_flat)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURE_DIR, 'roc_curve.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055f851e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot PR curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_true_flat, y_pred_flat)\n",
        "average_precision = average_precision_score(y_true_flat, y_pred_flat)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(recalls, precisions, lw=2, label=f\"AP = {average_precision:.4f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision–Recall Curve\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURE_DIR, 'pr_curve.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4784c95",
      "metadata": {},
      "source": [
        "# Reference List\n",
        "\n",
        "- Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep Learning. [online] pp.1–23. Available at: <https://mitpress.mit.edu/9780262035613/deep-learning/> [Accessed 17 May 2025].\n",
        "- He, H. and Garcia, E.A., 2009. Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering, [online] 21(9), pp.1263–1284. https://doi.org/10.1109/TKDE.2008.239.\n",
        "- Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. ImageNet Classification with Deep Convolutional Neural Networks. [online] Available at: <http://code.google.com/p/cuda-convnet/>.\n",
        "- Lecun, Y., Bengio, Y. and Hinton, G., 2015. Deep learning. Nature, https://doi.org/10.1038/nature14539.\n",
        "- Rawat, W. and Wang, Z., 2017. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural computation, [online] 29(9), pp.2352–2449. https://doi.org/10.1162/NECO_A_00990.\n",
        "- Shorten, C. and Khoshgoftaar, T.M., 2019. A survey on Image Data Augmentation for Deep Learning. Journal of Big Data 2019 6:1, [online] 6(1), pp.60-. https://doi.org/10.1186/S40537-019-0197-0.\n",
        "- Srivastava, N., Hinton, G., Krizhevsky, A. and Salakhutdinov, R., 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, .\n",
        " \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
